{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A more precise measurement, such as a urine al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the symptoms of Fraser like syndrome ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Most cases of shingles occur in adults. Only a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is Bilateral perisylvian polymicrogyria inheri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\",GARD,Coccygodynia What are the treatments fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77802</th>\n",
       "      <td>In the fatal cases (4/5 patients), a transient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77803</th>\n",
       "      <td>This chemical robs your blood of oxygen and tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77804</th>\n",
       "      <td>Physical Activity Being physically active and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77805</th>\n",
       "      <td>\",GARD,Faciocardiorenal syndrome What is (are)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77806</th>\n",
       "      <td>\",GARD,3-Hydroxyisobutyric aciduria Is 3-Hydro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77807 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  corpus\n",
       "0      A more precise measurement, such as a urine al...\n",
       "1      What are the symptoms of Fraser like syndrome ...\n",
       "2      Most cases of shingles occur in adults. Only a...\n",
       "3      Is Bilateral perisylvian polymicrogyria inheri...\n",
       "4      \",GARD,Coccygodynia What are the treatments fo...\n",
       "...                                                  ...\n",
       "77802  In the fatal cases (4/5 patients), a transient...\n",
       "77803  This chemical robs your blood of oxygen and tr...\n",
       "77804  Physical Activity Being physically active and ...\n",
       "77805  \",GARD,Faciocardiorenal syndrome What is (are)...\n",
       "77806  \",GARD,3-Hydroxyisobutyric aciduria Is 3-Hydro...\n",
       "\n",
       "[77807 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/dataset.csv')\n",
    "df['corpus'] = df['corpus'].astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A more precise measurement, such as a urine albumin-to-creatinine ratio, may be necessary to confirm kidney disease. Unlike a dipstick test for albumin, a urine albumin-to-creatinine ratiothe ratio between the amount of albumin and the amount of creatinine in urineis not affected by variation in urine concentration. Blood test.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = df['corpus'].values\n",
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  A more precise measurement, such as a urine albumin-to-creatinine ratio, may be necessary to confirm kidney disease. Unlike a dipstick test for albumin, a urine albumin-to-creatinine ratiothe ratio between the amount of albumin and the amount of creatinine in urineis not affected by variation in urine concentration. Blood test.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(samples))\n",
    "\n",
    "for txt in dataset.take(1):\n",
    "    print(\"Sentence: \", txt.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 8000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    dataset,\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w', encoding='utf-8') as file:\n",
    "    for token in vocab:\n",
    "        print(token, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7951"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.BertTokenizer('vocab.txt', **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128)\n",
      "(32, 128)\n",
      "Input: [ 144  156  649   96   97  271  291   86   85 1437   11  792 3422   12\n",
      "   17]\n",
      "Output: [ 156  649   96   97  271  291   86   85 1437   11  792 3422   12   17\n",
      "  216]\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 32\n",
    "MAX_TOKENS = 129\n",
    "\n",
    "def prepare_data(token):\n",
    "      token = token[:MAX_TOKENS]\n",
    "      input = token[:-1]\n",
    "      label = token[1:]\n",
    "      \n",
    "      return input, label\n",
    "\n",
    "train_dataset = tokenizer.tokenize(samples[:166021]).merge_dims(-2,-1).to_tensor()\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset)\n",
    "train_dataset = train_dataset.map(prepare_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tokenizer.tokenize(samples[166021:]).merge_dims(-2,-1).to_tensor()\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_dataset)\n",
    "val_dataset = val_dataset.map(prepare_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "for x_train, y_train in train_dataset.take(1):\n",
    "      break\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Input:', x_train[0][:15].numpy())\n",
    "print('Output:', y_train[0][:15].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1) \n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                        dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x)\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, target_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                            num_heads=num_heads, dff=dff,\n",
    "                            vocab_size=target_vocab_size,\n",
    "                            dropout_rate=dropout_rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "        # first argument.\n",
    "\n",
    "        x = self.decoder(x)  # (batch_size, target_len, d_model)\n",
    "\n",
    "        # Final linear layer output.\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        # Return the final output and the attention weights.\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "d_model = 64\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128)\n",
      "(32, 128, 7735)\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    target_vocab_size=len(vocab),\n",
    "    dropout_rate=dropout_rate)\n",
    "\n",
    "output = transformer(x_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder (Decoder)           multiple                  893120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  502775    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,395,895\n",
      "Trainable params: 1,395,895\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_masked_accuracy', patience=3)\n",
    "transformer.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5189/5189 [==============================] - 799s 153ms/step - loss: 3.4576 - masked_accuracy: 0.3349 - val_loss: 2.4372 - val_masked_accuracy: 0.4610\n",
      "Epoch 2/20\n",
      "5189/5189 [==============================] - 737s 142ms/step - loss: 2.3977 - masked_accuracy: 0.4598 - val_loss: 2.2214 - val_masked_accuracy: 0.4952\n",
      "Epoch 3/20\n",
      "5189/5189 [==============================] - 690s 133ms/step - loss: 2.2850 - masked_accuracy: 0.4772 - val_loss: 2.1511 - val_masked_accuracy: 0.5069\n",
      "Epoch 4/20\n",
      "5189/5189 [==============================] - 688s 133ms/step - loss: 2.2381 - masked_accuracy: 0.4850 - val_loss: 2.1165 - val_masked_accuracy: 0.5128\n",
      "Epoch 5/20\n",
      "5189/5189 [==============================] - 686s 132ms/step - loss: 2.2106 - masked_accuracy: 0.4898 - val_loss: 2.0928 - val_masked_accuracy: 0.5169\n",
      "Epoch 6/20\n",
      "5189/5189 [==============================] - 688s 133ms/step - loss: 2.1914 - masked_accuracy: 0.4932 - val_loss: 2.0728 - val_masked_accuracy: 0.5212\n",
      "Epoch 7/20\n",
      "5189/5189 [==============================] - 689s 133ms/step - loss: 2.1772 - masked_accuracy: 0.4960 - val_loss: 2.0605 - val_masked_accuracy: 0.5233\n",
      "Epoch 8/20\n",
      "5189/5189 [==============================] - 683s 131ms/step - loss: 2.1663 - masked_accuracy: 0.4980 - val_loss: 2.0511 - val_masked_accuracy: 0.5257\n",
      "Epoch 9/20\n",
      "5189/5189 [==============================] - 681s 131ms/step - loss: 2.1567 - masked_accuracy: 0.4997 - val_loss: 2.0438 - val_masked_accuracy: 0.5277\n",
      "Epoch 10/20\n",
      "5189/5189 [==============================] - 680s 131ms/step - loss: 2.1494 - masked_accuracy: 0.5013 - val_loss: 2.0356 - val_masked_accuracy: 0.5290\n",
      "Epoch 11/20\n",
      "5189/5189 [==============================] - 681s 131ms/step - loss: 2.1426 - masked_accuracy: 0.5026 - val_loss: 2.0231 - val_masked_accuracy: 0.5312\n",
      "Epoch 12/20\n",
      "5189/5189 [==============================] - 679s 131ms/step - loss: 2.1367 - masked_accuracy: 0.5038 - val_loss: 2.0235 - val_masked_accuracy: 0.5322\n",
      "Epoch 13/20\n",
      "5189/5189 [==============================] - 679s 131ms/step - loss: 2.1313 - masked_accuracy: 0.5048 - val_loss: 2.0121 - val_masked_accuracy: 0.5326\n",
      "Epoch 14/20\n",
      "5189/5189 [==============================] - 679s 131ms/step - loss: 2.1269 - masked_accuracy: 0.5057 - val_loss: 2.0065 - val_masked_accuracy: 0.5347\n",
      "Epoch 15/20\n",
      "5189/5189 [==============================] - 678s 131ms/step - loss: 2.1228 - masked_accuracy: 0.5065 - val_loss: 2.0026 - val_masked_accuracy: 0.5354\n",
      "Epoch 16/20\n",
      "5189/5189 [==============================] - 679s 131ms/step - loss: 2.1195 - masked_accuracy: 0.5071 - val_loss: 1.9993 - val_masked_accuracy: 0.5368\n",
      "Epoch 17/20\n",
      "5189/5189 [==============================] - 679s 131ms/step - loss: 2.1157 - masked_accuracy: 0.5078 - val_loss: 1.9969 - val_masked_accuracy: 0.5364\n",
      "Epoch 18/20\n",
      "5189/5189 [==============================] - 679s 131ms/step - loss: 2.1129 - masked_accuracy: 0.5084 - val_loss: 1.9904 - val_masked_accuracy: 0.5362\n",
      "Epoch 19/20\n",
      "5189/5189 [==============================] - 678s 131ms/step - loss: 2.1100 - masked_accuracy: 0.5090 - val_loss: 1.9895 - val_masked_accuracy: 0.5374\n",
      "Epoch 20/20\n",
      "5189/5189 [==============================] - 679s 131ms/step - loss: 2.1070 - masked_accuracy: 0.5096 - val_loss: 1.9892 - val_masked_accuracy: 0.5384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21735f76400>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(train_dataset, epochs=20, validation_data=val_dataset, callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.save_weights('./training_1/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2183ac3faf0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads the weights\n",
    "transformer.load_weights('./training_1/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(sentence, maxlen=MAX_TOKENS):\n",
    "    output_array = tokenizer.tokenize(sentence).merge_dims(-2, -1).to_tensor()\n",
    "    \n",
    "    for i in range(maxlen):\n",
    "        prediction = transformer(output_array, training=False)\n",
    "        prediction = prediction[:, -1:, :]\n",
    "        prediction = tf.argmax(prediction, axis=-1)\n",
    "        output_array = tf.concat([output_array, prediction], axis=1)\n",
    "        \n",
    "        if prediction[0][0].numpy() == 3:\n",
    "            break\n",
    "        \n",
    "    output = tokenizer.detokenize(output_array).to_tensor()\n",
    "    output = ' '.join([word.decode('utf-8') for word in output.numpy()[0]])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"what is type 2 diabetes ? diabetes is a condition in which the body ' s cells are able to produce insulin . when the body ' s cells are able to use insulin , the body ' s cells , the body ' s cells , and the cells are able to use insulin . insulin resistance is a condition in which the body ' s cells are able to use insulin effectively . insulin resistance is a condition in which the body ' s cells are able to use insulin effectively . insulin resistance is a condition in which the body ' s cells are able to use insulin effectively . insulin resistance is a hormone that helps control insulin production . insulin resistance is a hormone that is a hormone that\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('what is type 2 diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'characteristic of x - linked inheritance is that fathers cannot pass x - linked traits to their sons . \" , ghr , x - linked traits to x - linked traits to their sons what are the treatments for x - linked inheritance ? , these resources address the diagnosis or management of x - linked recessive inheritance : - gene review : gene review : x - linked recessive inheritance - gene review : gene review : x - linked recessive disorders - genetic testing registry : x - linked recessive disorders - medlineplus encyclopedia : x - linked recessive disorders these resources from medlineplus offer information about the diagnosis and management of various health conditions : - diagnostic tests - drug therapy - surgery and rehabilitation - genetic counseling - palliative care'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('characteristic of X-linked inheritance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('transformer_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5ef8c4b80788ce44ced42bcb28e414f75089d47a94a3a967df09102d2c56b2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
